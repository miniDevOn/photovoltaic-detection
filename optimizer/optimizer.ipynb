{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "source": [
    "# Appropriate Optimizers\n",
    "I am confident that for this project, Adam Optimizer is the best in term of accuracy and efficiency. Here is the code for Adam, and SGD if we want to compare Adam to SGD"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters(, lr=0.01)"
   ]
  },
  {
   "source": [
    "# Motivations\n",
    "## State-of-the-Art CNN Optimizer for Brain Tumor\n",
    "https://sci-hub.se/10.3390/brainsci10070427\n",
    "In this paper they compare a dozen of gradient based methods, with multiple learning rates, number of epochs. They measure the accuracy and loss with CV. They constantly got that Adam was best suited. \n",
    "\n",
    "*The smallest error rate was obtained by the Adam optimizer using our proposed CNN architecture. Adam is the most successful optimizer in our all experiments. Other optimizers also performed well, and the performances of SGD and momentum are close to that of Adam. In any case, Adam is the most steady one among the ten\n",
    "optimizers. Then again, NAG, RMSProp, and CLR were not successful to do as such. The error rate ofSGD and momentum tended to diminish when the learning rate was smaller than 1eâˆ’5. Compared to Nadam, Adagrade and AdaDelta have worse performance. Therefore, **Adam is the best choice** for brain tumor segmentation using our proposed CNN architecture*\n",
    "\n",
    "## Satellite Image Segmentation for Building Detection\n",
    "*First\n",
    "of, we replaced the stochastic gradient descent with the Adam Optimizer, known to converge faster during training.*\n",
    "\n",
    "## Deep learning in the built environment: automatic detection of rooftop\n",
    "\n",
    "*We test Adam and\n",
    "Stochastic Gradient Descent optimizers [20], seeing better convergence with Adam, with a learning rate of 0.1.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}