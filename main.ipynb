{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting rooftop available surface for installing PV modules in aerial images using Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot  as plt\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "from process_data.data_noara_loader import *\n",
    "from model.unet import *\n",
    "from loss.loss import *\n",
    "from process_data.data_loader import *\n",
    "from process_data.data_noara_loader import *\n",
    "from hyperparameters.select_param import *\n",
    "from process_data.import_test import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data Set\n",
    "First we load the data set that we will use for training. Each sample is an image with its mask (label). An image is represented as a 3x250x250 array with each of the 3 color chanel being 250x250 pixels. The asssociated mask is a 250x250 array, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_image = 'data/image'\n",
    "folder_path_mask  = 'data/mask'\n",
    "folder_path_noara  = 'data/noARA'\n",
    "\n",
    "#load dataset\n",
    "train_dataset = ConcatDataset([DataLoaderSegmentation(folder_path_image,folder_path_mask),DataLoaderNoARA(folder_path_noara)])\n",
    "\n",
    "#combine two datasets\n",
    "train_loader = DataLoader(train_dataset,batch_size=5, shuffle=True ,num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate the model\n",
    "In this report, we will use the Unet model presented in medical image segmentation, and in the previous papers of the Professor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "UNet(\n  (inc): inconv(\n    (conv): double_conv(\n      (conv): Sequential(\n        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (down1): down(\n    (mpconv): Sequential(\n      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): double_conv(\n        (conv): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n  )\n  (down2): down(\n    (mpconv): Sequential(\n      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): double_conv(\n        (conv): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n  )\n  (down3): down(\n    (mpconv): Sequential(\n      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): double_conv(\n        (conv): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n  )\n  (down4): down(\n    (mpconv): Sequential(\n      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): double_conv(\n        (conv): Sequential(\n          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n  )\n  (up1): up(\n    (up): ConvTranspose2d(512, 512, kernel_size=(2, 2), stride=(2, 2))\n    (conv): double_conv(\n      (conv): Sequential(\n        (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (up2): up(\n    (up): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n    (conv): double_conv(\n      (conv): Sequential(\n        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (up3): up(\n    (up): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n    (conv): double_conv(\n      (conv): Sequential(\n        (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (up4): up(\n    (up): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n    (conv): double_conv(\n      (conv): Sequential(\n        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (outc): outconv(\n    (conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (activation): Sigmoid()\n)\n"
     ]
    }
   ],
   "source": [
    "model = UNet(3,1,False).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "model = UNet(3,1,False).to(device)\n",
    "loss_function = torch.nn.BCEWithLogitsLoss(weight=torch.FloatTensor([6]).cuda())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "trained_model = training_model(train_loader,loss_function,optimizer,model,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model/trained_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'model/'+input('Name of the model file:')\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "source": [
    "# Training with learning rate decay"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "loss_function = torch.nn.BCEWithLogitsLoss(weight=torch.FloatTensor([5]).cuda())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.5, last_epoch=-1, verbose=True)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 10, eta_min=0, last_epoch=-1, verbose=True)\n",
    "\n",
    "trained_model = training_model(train_loader,loss_function,optimizer,model,num_epochs,scheduler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "num_epochs = 2\n",
    "lr = 0.01\n",
    "\n",
    "iou_, acc_ = cross_validation(train_dataset, loss_function, input_model, num_epochs, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning the hyper parameters\n",
    "We may do a grid search on the learning rates with cross validation to find the best learning_rate. For now the used metric is iou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---------------------------------------------------------------------\n",
      "\n",
      "Learning Rate = 0.1\n",
      "\n",
      "Epoch n. 0 Loss 0.9327 Time Remaining 40.9744\n",
      "Epoch n. 10 Loss 0.6682 Time Remaining 34.7927\n",
      "Epoch n. 20 Loss 0.572 Time Remaining 29.0681\n",
      "Epoch n. 30 Loss 0.4497 Time Remaining 23.2385\n",
      "Epoch n. 40 Loss 0.4403 Time Remaining 17.8538\n",
      "Epoch n. 50 Loss 0.2982 Time Remaining 11.8843\n",
      "Epoch n. 60 Loss 0.4266 Time Remaining 5.9444\n",
      "Iter 0: IoU = 0.1096 /  Accuracy = 0.04474\n",
      "Epoch n. 0 Loss 0.3218 Time Remaining 41.5957\n",
      "Epoch n. 10 Loss 0.3194 Time Remaining 35.6762\n",
      "Epoch n. 20 Loss 0.236 Time Remaining 29.0631\n",
      "Epoch n. 30 Loss 0.393 Time Remaining 23.2472\n",
      "Epoch n. 40 Loss 0.1966 Time Remaining 17.8438\n",
      "Epoch n. 50 Loss 0.2243 Time Remaining 11.8925\n",
      "Epoch n. 60 Loss 0.1686 Time Remaining 5.8153\n",
      "Iter 1: IoU = 0.1194 /  Accuracy = 0.02846\n",
      "Epoch n. 0 Loss 0.268 Time Remaining 41.6821\n",
      "Epoch n. 10 Loss 0.1851 Time Remaining 34.9218\n",
      "Epoch n. 20 Loss 0.2404 Time Remaining 29.8051\n",
      "Epoch n. 30 Loss 0.1764 Time Remaining 23.2872\n",
      "Epoch n. 40 Loss 0.1383 Time Remaining 17.4599\n",
      "Epoch n. 50 Loss 0.132 Time Remaining 11.9242\n",
      "Epoch n. 60 Loss 0.1176 Time Remaining 5.9563\n",
      "Iter 2: IoU = 0.09678 /  Accuracy = 0.01802\n",
      "Epoch n. 0 Loss 0.1505 Time Remaining 41.7078\n",
      "Epoch n. 10 Loss 0.1185 Time Remaining 35.7656\n",
      "Epoch n. 20 Loss 0.107 Time Remaining 29.788\n",
      "Epoch n. 30 Loss 0.0906 Time Remaining 23.2805\n",
      "Epoch n. 40 Loss 0.1005 Time Remaining 17.4662\n",
      "Epoch n. 50 Loss 0.091 Time Remaining 11.9017\n",
      "Epoch n. 60 Loss 0.0735 Time Remaining 5.82\n",
      "Iter 3: IoU = 0.1086 /  Accuracy = 0.009813\n",
      "Epoch n. 0 Loss 0.0918 Time Remaining 40.7514\n",
      "Epoch n. 10 Loss 0.0805 Time Remaining 32.7228\n",
      "Epoch n. 20 Loss 0.0683 Time Remaining 27.2515\n",
      "Epoch n. 30 Loss 0.0649 Time Remaining 21.7938\n",
      "Epoch n. 40 Loss 0.0608 Time Remaining 16.3549\n",
      "Epoch n. 50 Loss 0.0596 Time Remaining 10.8993\n",
      "Epoch n. 60 Loss 0.0571 Time Remaining 5.4503\n",
      "Iter 4: IoU = 0.1169 /  Accuracy = 0.007506\n",
      "\n",
      "Average test IoU: 0.110244\n",
      "Variance test IoU: 0.000063\n",
      "\n",
      "Average test accuracy: 0.021708\n",
      "Variance test accuracy: 0.000187\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Learning Rate = 0.05623413251903491\n",
      "\n",
      "Epoch n. 0 Loss 0.0609 Time Remaining 38.0657\n",
      "Epoch n. 10 Loss 0.0549 Time Remaining 32.6377\n",
      "Epoch n. 20 Loss 0.0536 Time Remaining 27.2089\n",
      "Epoch n. 30 Loss 0.0516 Time Remaining 21.7645\n",
      "Epoch n. 40 Loss 0.0507 Time Remaining 16.3564\n",
      "Epoch n. 50 Loss 0.052 Time Remaining 10.8866\n",
      "Epoch n. 60 Loss 0.0546 Time Remaining 5.4393\n",
      "Iter 0: IoU = 0.1032 /  Accuracy = 0.005825\n",
      "Epoch n. 0 Loss 0.057 Time Remaining 38.0855\n",
      "Epoch n. 10 Loss 0.048 Time Remaining 32.6527\n",
      "Epoch n. 20 Loss 0.0472 Time Remaining 27.2039\n",
      "Epoch n. 30 Loss 0.0455 Time Remaining 21.7752\n",
      "Epoch n. 40 Loss 0.0446 Time Remaining 16.3294\n",
      "Epoch n. 50 Loss 0.0433 Time Remaining 10.8889\n",
      "Epoch n. 60 Loss 0.0444 Time Remaining 5.4415\n",
      "Iter 1: IoU = 0.1177 /  Accuracy = 0.00608\n",
      "Epoch n. 0 Loss 0.0468 Time Remaining 38.1287\n",
      "Epoch n. 10 Loss 0.0443 Time Remaining 32.6988\n",
      "Epoch n. 20 Loss 0.0425 Time Remaining 27.249\n",
      "Epoch n. 30 Loss 0.0419 Time Remaining 21.8099\n",
      "Epoch n. 40 Loss 0.0408 Time Remaining 16.3499\n",
      "Epoch n. 50 Loss 0.0409 Time Remaining 10.9046\n",
      "Epoch n. 60 Loss 0.0393 Time Remaining 5.4525\n",
      "Iter 2: IoU = 0.1028 /  Accuracy = 0.004395\n",
      "Epoch n. 0 Loss 0.0398 Time Remaining 38.131\n",
      "Epoch n. 10 Loss 0.038 Time Remaining 32.7008\n",
      "Epoch n. 20 Loss 0.0366 Time Remaining 27.2365\n",
      "Epoch n. 30 Loss 0.0361 Time Remaining 21.7945\n",
      "Epoch n. 40 Loss 0.0359 Time Remaining 16.3529\n",
      "Epoch n. 50 Loss 0.0344 Time Remaining 10.9036\n",
      "Epoch n. 60 Loss 0.0341 Time Remaining 5.45\n",
      "Iter 3: IoU = 0.1225 /  Accuracy = 0.004188\n",
      "Epoch n. 0 Loss 0.0366 Time Remaining 38.1684\n",
      "Epoch n. 10 Loss 0.0356 Time Remaining 32.7038\n",
      "Epoch n. 20 Loss 0.0355 Time Remaining 27.2565\n",
      "Epoch n. 30 Loss 0.0335 Time Remaining 21.7965\n",
      "Epoch n. 40 Loss 0.0325 Time Remaining 16.3419\n",
      "Epoch n. 50 Loss 0.0322 Time Remaining 10.9033\n",
      "Epoch n. 60 Loss 0.031 Time Remaining 5.4503\n",
      "Iter 4: IoU = 0.1124 /  Accuracy = 0.003789\n",
      "\n",
      "Average test IoU: 0.111724\n",
      "Variance test IoU: 0.000061\n",
      "\n",
      "Average test accuracy: 0.004856\n",
      "Variance test accuracy: 0.000001\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "Learning Rate = 0.03162277660168379\n",
      "\n",
      "Epoch n. 0 Loss 0.0302 Time Remaining 38.0925\n",
      "Epoch n. 10 Loss 0.0293 Time Remaining 32.6858\n",
      "Epoch n. 20 Loss 0.0283 Time Remaining 27.2164\n",
      "Epoch n. 30 Loss 0.0283 Time Remaining 21.7645\n",
      "Epoch n. 40 Loss 0.0276 Time Remaining 16.3324\n",
      "Epoch n. 50 Loss 0.0272 Time Remaining 10.8899\n",
      "Epoch n. 60 Loss 0.0284 Time Remaining 5.4416\n",
      "Iter 0: IoU = 0.122 /  Accuracy = 0.003742\n",
      "Epoch n. 0 Loss 0.0303 Time Remaining 38.0785\n",
      "Epoch n. 10 Loss 0.0294 Time Remaining 32.6688\n",
      "Epoch n. 20 Loss 0.0287 Time Remaining 27.224\n",
      "Epoch n. 30 Loss 0.0283 Time Remaining 21.7705\n",
      "Epoch n. 40 Loss 0.0278 Time Remaining 16.3264\n",
      "Epoch n. 50 Loss 0.0276 Time Remaining 10.8856\n",
      "Epoch n. 60 Loss 0.0284 Time Remaining 5.4458\n",
      "Iter 1: IoU = 0.1097 /  Accuracy = 0.002729\n",
      "Epoch n. 0 Loss 0.0282 Time Remaining 41.7747\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c2a95a7643d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0minput_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mbest_lr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_iou\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_hyper_param\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Raphael\\Documents\\GitHub\\rooftop-cnn-detection\\hyperparameters\\select_param.py\u001b[0m in \u001b[0;36mselect_hyper_param\u001b[1;34m(train_dataset, loss_function, input_model, num_epochs, lr_candidates)\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'---------------------------------------------------------------------\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Learning Rate = {}\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[0miou\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m         \u001b[0mcomparison\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miou\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[0mcomparison\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomparison\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Raphael\\Documents\\GitHub\\rooftop-cnn-detection\\hyperparameters\\select_param.py\u001b[0m in \u001b[0;36mcross_validation\u001b[1;34m(train_dataset, loss_function, input_model, num_epochs, lr)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;31m#train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_fold_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[1;31m# make prediction and compute the evaluation metrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0miou\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_fold_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Raphael\\Documents\\GitHub\\rooftop-cnn-detection\\hyperparameters\\select_param.py\u001b[0m in \u001b[0;36mtraining_model\u001b[1;34m(train_loader, loss_function, optimizer, model, num_epochs, scheduler)\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr_candidates = np.logspace(-2,-3,3)\n",
    "num_epochs = 70\n",
    "loss_function = torch.nn.BCEWithLogitsLoss(pos_weight=torch.FloatTensor([6]).cuda())\n",
    "\n",
    "input_model = UNet(3,1,False).to(device)\n",
    "\n",
    "best_lr, best_model, best_iou = select_hyper_param(train_dataset,loss_function,input_model,num_epochs,lr_candidates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr, best_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model.state_dict(), 'model/best_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display a image with its mask"
   ]
  },
  {
   "source": [
    "fig = plt.figure()\n",
    "fig.set_size_inches(12, 7, forward=True)\n",
    "\n",
    "ax1 = fig.add_subplot(1,3,1)\n",
    "ax1.title.set_text('Input Image')\n",
    "ax2 = fig.add_subplot(1,3,2)\n",
    "ax2.title.set_text('Expected Label')\n",
    "ax3 = fig.add_subplot(1,3,3)\n",
    "ax3.title.set_text('Predicted Label')\n",
    "\n",
    "acc = 0\n",
    "\n",
    "index_random_sample = int(np.random.random()*len(train_loader.dataset))\n",
    "(x,y) = train_loader.dataset[index_random_sample]\n",
    "ax1.imshow(np.transpose(x.numpy(),(1,2,0)))\n",
    "\n",
    "ax2.imshow(y)\n",
    "\n",
    "ypred = torch.squeeze(model(torch.unsqueeze(x,0).cuda())).cpu().detach().numpy()\n",
    "ax3.imshow(np.around(ypred))\n",
    "np.around(iou(np.around(ypred),y.numpy()),4),accuracy(np.around(ypred),y.numpy())\n",
    "acc = np.around(iou(np.around(ypred),y.numpy()),4)\n",
    "plt.show()\n",
    "print(acc,accuracy(np.around(ypred),y.numpy()))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## Display an unseen image"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_and_show(model,'test.png') # Note that 'test.png' should be located in the root of the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}